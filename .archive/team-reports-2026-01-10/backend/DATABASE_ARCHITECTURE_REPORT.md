# æ•°æ®åº“æ¶æ„è¯„ä¼°æŠ¥å‘Šä¸ PostgreSQL è¿ç§»æ–¹æ¡ˆ

**é¡¹ç›®**: HelloAgents Platform
**å½“å‰æ•°æ®åº“**: SQLite 3
**ç›®æ ‡æ•°æ®åº“**: PostgreSQL 17+
**è¯„ä¼°æ—¥æœŸ**: 2026-01-10
**è¯„ä¼°äºº**: Database Architect

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### å½“å‰çŠ¶æ€
- **æ•°æ®åº“ç±»å‹**: SQLite (å¼€å‘ç¯å¢ƒ)
- **æ•°æ®åº“å¤§å°**: 1.3 MB
- **è¡¨æ•°é‡**: 5 ä¸ªæ ¸å¿ƒè¡¨
- **æ•°æ®é‡**:
  - users: 1 æ¡è®°å½•
  - lessons: 18 æ¡è®°å½•
  - user_progress: 0 æ¡è®°å½•
  - code_submissions: 0 æ¡è®°å½•
  - chat_messages: 0 æ¡è®°å½•
- **ç´¢å¼•ä¼˜åŒ–**: å·²éƒ¨åˆ†ä¼˜åŒ–ï¼ˆå¤åˆç´¢å¼•è¦†ç›–å¸¸è§æŸ¥è¯¢ï¼‰

### è¿ç§»å»ºè®®
- **è¿ç§»æ—¶æœº**: é€‚åˆè¿ç§»ï¼ˆæ•°æ®é‡å°ï¼Œé£é™©ä½ï¼‰
- **é¢„æœŸåœæœºæ—¶é—´**: < 5 åˆ†é’Ÿï¼ˆé›¶åœæœºæ–¹æ¡ˆå¯é€‰ï¼‰
- **é£é™©ç­‰çº§**: ğŸŸ¢ ä½é£é™©
- **ä¼˜å…ˆçº§**: ä¸­ç­‰ï¼ˆç”Ÿäº§ç¯å¢ƒå¿…é¡»ï¼Œå¼€å‘ç¯å¢ƒå¯é€‰ï¼‰

---

## 1. æ•°æ®æ¨¡å‹è¯„ä¼°

### 1.1 User æ¨¡å‹ âœ… è‰¯å¥½

```python
class User(Base):
    id: BIGSERIAL PRIMARY KEY           # âœ… é€‚åˆè¿ç§»
    username: VARCHAR(50) UNIQUE        # âœ… ç´¢å¼•å®Œå–„
    full_name: VARCHAR(100)             # âœ…
    settings: TEXT (JSON)               # âš ï¸ å»ºè®®æ”¹ä¸º JSONB (PostgreSQL)
    created_at: TIMESTAMPTZ             # âš ï¸ SQLite ä½¿ç”¨ VARCHAR
    updated_at: TIMESTAMPTZ             # âš ï¸ SQLite ä½¿ç”¨ VARCHAR
    last_login: TIMESTAMPTZ             # âš ï¸ SQLite ä½¿ç”¨ VARCHAR
```

**ä¼˜ç‚¹**:
- ç”¨æˆ·åæœ‰å”¯ä¸€ç´¢å¼•ï¼ŒæŸ¥è¯¢æ€§èƒ½è‰¯å¥½
- å…³ç³»å®šä¹‰æ¸…æ™°ï¼ˆçº§è”åˆ é™¤ï¼‰

**æ”¹è¿›å»ºè®®**:
1. **æ—¶é—´æˆ³ç±»å‹ä¼˜åŒ–**: å°† VARCHAR æ”¹ä¸º TIMESTAMPTZ
2. **JSON å­—æ®µä¼˜åŒ–**: å°† TEXT æ”¹ä¸º JSONBï¼Œæ”¯æŒé«˜æ•ˆæŸ¥è¯¢
3. **æ·»åŠ è§¦å‘å™¨**: è‡ªåŠ¨æ›´æ–° `updated_at` å­—æ®µ
4. **æ·»åŠ çº¦æŸ**: é‚®ç®±æ ¼å¼éªŒè¯ï¼ˆå¦‚æœå°†æ¥æ·»åŠ ï¼‰

---

### 1.2 Lesson æ¨¡å‹ âœ… è‰¯å¥½

```python
class Lesson(Base):
    id: BIGSERIAL PRIMARY KEY
    chapter_number: INTEGER NOT NULL    # âœ… æœ‰ç´¢å¼•
    lesson_number: INTEGER NOT NULL
    title: VARCHAR(200) NOT NULL
    content: TEXT NOT NULL              # âš ï¸ è€ƒè™‘å‹ç¼©å­˜å‚¨
    starter_code: TEXT                  # âš ï¸ è€ƒè™‘å‹ç¼©å­˜å‚¨
    extra_data: TEXT (JSON)             # âš ï¸ å»ºè®®æ”¹ä¸º JSONB
    created_at: TIMESTAMPTZ
    updated_at: TIMESTAMPTZ

    CONSTRAINT uk_chapter_lesson UNIQUE (chapter_number, lesson_number)
```

**ä¼˜ç‚¹**:
- å”¯ä¸€çº¦æŸä¿è¯è¯¾ç¨‹ç¼–å·ä¸é‡å¤
- å•å­—æ®µç´¢å¼• `chapter_number` æ”¯æŒæŒ‰ç« èŠ‚æŸ¥è¯¢

**æ”¹è¿›å»ºè®®**:
1. **æ·»åŠ å…¨æ–‡æœç´¢ç´¢å¼•**: åœ¨ `title` å’Œ `content` ä¸Šåˆ›å»º GIN ç´¢å¼•
2. **å†…å®¹å‹ç¼©**: ä½¿ç”¨ PostgreSQL çš„ TOAST è‡ªåŠ¨å‹ç¼©å¤§æ–‡æœ¬
3. **æ·»åŠ æšä¸¾ç±»å‹**: éš¾åº¦ç­‰çº§ã€è¯¾ç¨‹ç±»å‹ç­‰
4. **æ·»åŠ ç‰ˆæœ¬æ§åˆ¶**: è¿½è¸ªè¯¾ç¨‹å†…å®¹ä¿®æ”¹å†å²

---

### 1.3 UserProgress æ¨¡å‹ âœ… ä¼˜ç§€

```python
class UserProgress(Base):
    id: BIGSERIAL PRIMARY KEY
    user_id: INTEGER NOT NULL           # âœ… å¤–é”® + ç´¢å¼•
    lesson_id: INTEGER NOT NULL         # âœ… å¤–é”® + ç´¢å¼•
    completed: BOOLEAN                  # âš ï¸ SQLite ä½¿ç”¨ INTEGER
    current_code: TEXT                  # âš ï¸ è€ƒè™‘å‹ç¼©æˆ–å•ç‹¬è¡¨
    cursor_position: TEXT (JSON)        # âš ï¸ å»ºè®®æ”¹ä¸º JSONB
    started_at: TIMESTAMPTZ
    completed_at: TIMESTAMPTZ
    last_accessed: TIMESTAMPTZ

    CONSTRAINT uk_user_lesson UNIQUE (user_id, lesson_id)

    # å¤åˆç´¢å¼•ï¼ˆä¼˜ç§€è®¾è®¡ï¼‰
    INDEX idx_user_completed (user_id, completed)
    INDEX idx_user_last_accessed (user_id, last_accessed)
    INDEX idx_lesson_completed (lesson_id, completed)
    INDEX idx_user_completed_accessed (user_id, completed, last_accessed)
```

**ä¼˜ç‚¹**:
- å”¯ä¸€çº¦æŸé˜²æ­¢é‡å¤è¿›åº¦è®°å½•
- å¤åˆç´¢å¼•è¦†ç›–ä¸»è¦æŸ¥è¯¢åœºæ™¯
- å¤–é”®çº§è”åˆ é™¤ä¿è¯æ•°æ®ä¸€è‡´æ€§

**æ”¹è¿›å»ºè®®**:
1. **åˆ†ç¦»ä»£ç å­˜å‚¨**: å°† `current_code` ç§»åˆ°å•ç‹¬çš„è¡¨ï¼ˆå‡å°‘ä¸»è¡¨å¤§å°ï¼‰
2. **æ·»åŠ ç»Ÿè®¡å­—æ®µ**: å­¦ä¹ æ—¶é•¿ã€å°è¯•æ¬¡æ•°ç­‰
3. **æ·»åŠ åˆ†åŒº**: æŒ‰æ—¶é—´åˆ†åŒºï¼ˆæœªæ¥æ•°æ®é‡å¢é•¿æ—¶ï¼‰

---

### 1.4 CodeSubmission æ¨¡å‹ âœ… ä¼˜ç§€

```python
class CodeSubmission(Base):
    id: BIGSERIAL PRIMARY KEY
    user_id: INTEGER NOT NULL
    lesson_id: INTEGER NOT NULL
    code: TEXT NOT NULL                 # âš ï¸ è€ƒè™‘å‹ç¼©
    output: TEXT                        # âš ï¸ è€ƒè™‘å‹ç¼©
    status: VARCHAR(20) NOT NULL        # âœ… æœ‰ CHECK çº¦æŸ
    execution_time: FLOAT
    submitted_at: TIMESTAMPTZ

    CONSTRAINT chk_status CHECK (status IN ('success', 'error', 'timeout'))

    # å¤åˆç´¢å¼•ï¼ˆä¼˜ç§€è®¾è®¡ï¼‰
    INDEX idx_submission_user_lesson (user_id, lesson_id)
    INDEX idx_submission_user_submitted (user_id, submitted_at)
    INDEX idx_submission_lesson_submitted (lesson_id, submitted_at)
    INDEX idx_submission_lesson_user_status (lesson_id, user_id, status)
```

**ä¼˜ç‚¹**:
- å¤åˆç´¢å¼•è¦†ç›–ç»Ÿè®¡æŸ¥è¯¢ï¼ˆæˆåŠŸç‡ã€æäº¤å†å²ï¼‰
- CHECK çº¦æŸä¿è¯çŠ¶æ€æœ‰æ•ˆæ€§
- è®°å½•æ‰§è¡Œæ—¶é—´ä¾¿äºæ€§èƒ½åˆ†æ

**æ”¹è¿›å»ºè®®**:
1. **åˆ†åŒºè¡¨**: æŒ‰æäº¤æ—¶é—´åˆ†åŒºï¼ˆæŒ‰æœˆæˆ–æŒ‰å­£åº¦ï¼‰
2. **å½’æ¡£ç­–ç•¥**: å®šæœŸå½’æ¡£æ—§æ•°æ®åˆ°å†·å­˜å‚¨
3. **å‹ç¼©å­˜å‚¨**: ä»£ç å’Œè¾“å‡ºä½¿ç”¨ TOAST å‹ç¼©
4. **æ·»åŠ é”™è¯¯åˆ†ç±»**: åŒºåˆ†è¯­æ³•é”™è¯¯ã€è¿è¡Œæ—¶é”™è¯¯ã€è¶…æ—¶ç­‰

---

### 1.5 ChatMessage æ¨¡å‹ âœ… ä¼˜ç§€

```python
class ChatMessage(Base):
    id: BIGSERIAL PRIMARY KEY
    user_id: INTEGER NOT NULL
    lesson_id: INTEGER                  # âœ… å¯é€‰å¤–é”®
    role: VARCHAR(20) NOT NULL          # âœ… æœ‰ CHECK çº¦æŸ
    content: TEXT NOT NULL              # âš ï¸ è€ƒè™‘å‹ç¼©
    extra_data: TEXT (JSON)             # âš ï¸ å»ºè®®æ”¹ä¸º JSONB
    created_at: TIMESTAMPTZ

    CONSTRAINT chk_role CHECK (role IN ('user', 'assistant', 'system'))

    # å¤åˆç´¢å¼•ï¼ˆä¼˜ç§€è®¾è®¡ï¼‰
    INDEX idx_chat_user_created (user_id, created_at)
    INDEX idx_chat_user_lesson (user_id, lesson_id)
    INDEX idx_chat_lesson_created (lesson_id, created_at)
    INDEX idx_chat_user_lesson_created (user_id, lesson_id, created_at)
```

**ä¼˜ç‚¹**:
- ç´¢å¼•æ”¯æŒæŒ‰æ—¶é—´å€’åºæŸ¥è¯¢æœ€è¿‘å¯¹è¯
- CHECK çº¦æŸä¿è¯è§’è‰²æœ‰æ•ˆæ€§
- è½¯åˆ é™¤è®¾è®¡ï¼ˆON DELETE SET NULLï¼‰

**æ”¹è¿›å»ºè®®**:
1. **åˆ†åŒºè¡¨**: æŒ‰åˆ›å»ºæ—¶é—´åˆ†åŒºï¼ˆæŒ‰æœˆï¼‰
2. **ä¼šè¯ç®¡ç†**: æ·»åŠ  `session_id` åˆ†ç»„å¯¹è¯
3. **å…¨æ–‡æœç´¢**: åœ¨ `content` ä¸Šåˆ›å»º GIN ç´¢å¼•
4. **æ•°æ®å½’æ¡£**: è‡ªåŠ¨å½’æ¡£ 3 ä¸ªæœˆå‰çš„å¯¹è¯

---

## 2. ç´¢å¼•ä¼˜åŒ–è¯„ä¼° âœ… ä¼˜ç§€

### 2.1 ç°æœ‰ç´¢å¼•åˆ†æ

#### å•å­—æ®µç´¢å¼•
```sql
-- âœ… å¿…è¦ç´¢å¼•
CREATE UNIQUE INDEX ix_users_username ON users(username);
CREATE INDEX ix_lessons_chapter_number ON lessons(chapter_number);

-- âš ï¸ å¯èƒ½å†—ä½™çš„ç´¢å¼•ï¼ˆå·²æœ‰å¤åˆç´¢å¼•è¦†ç›–ï¼‰
CREATE INDEX ix_user_progress_user_id ON user_progress(user_id);
CREATE INDEX ix_user_progress_lesson_id ON user_progress(lesson_id);
CREATE INDEX ix_code_submissions_user_id ON code_submissions(user_id);
CREATE INDEX ix_code_submissions_lesson_id ON code_submissions(lesson_id);
CREATE INDEX ix_chat_messages_user_id ON chat_messages(user_id);
CREATE INDEX ix_chat_messages_lesson_id ON chat_messages(lesson_id);
```

#### å¤åˆç´¢å¼• âœ… è®¾è®¡ä¼˜ç§€
```sql
-- UserProgress ç´¢å¼•ï¼ˆè¦†ç›–ä¸»è¦æŸ¥è¯¢ï¼‰
CREATE INDEX idx_user_completed ON user_progress(user_id, completed);
CREATE INDEX idx_user_last_accessed ON user_progress(user_id, last_accessed);
CREATE INDEX idx_lesson_completed ON user_progress(lesson_id, completed);
CREATE INDEX idx_user_completed_accessed ON user_progress(user_id, completed, last_accessed);

-- CodeSubmission ç´¢å¼•ï¼ˆè¦†ç›–ç»Ÿè®¡æŸ¥è¯¢ï¼‰
CREATE INDEX idx_submission_user_lesson ON code_submissions(user_id, lesson_id);
CREATE INDEX idx_submission_user_submitted ON code_submissions(user_id, submitted_at);
CREATE INDEX idx_submission_lesson_submitted ON code_submissions(lesson_id, submitted_at);
CREATE INDEX idx_submission_lesson_user_status ON code_submissions(lesson_id, user_id, status);

-- ChatMessage ç´¢å¼•ï¼ˆè¦†ç›–å¯¹è¯æŸ¥è¯¢ï¼‰
CREATE INDEX idx_chat_user_created ON chat_messages(user_id, created_at);
CREATE INDEX idx_chat_user_lesson ON chat_messages(user_id, lesson_id);
CREATE INDEX idx_chat_lesson_created ON chat_messages(lesson_id, created_at);
CREATE INDEX idx_chat_user_lesson_created ON chat_messages(user_id, lesson_id, created_at);
```

### 2.2 ç´¢å¼•ä¼˜åŒ–å»ºè®®

#### ç§»é™¤å†—ä½™ç´¢å¼•
```sql
-- PostgreSQL è¿ç§»æ—¶ç§»é™¤ä»¥ä¸‹å•å­—æ®µç´¢å¼•ï¼ˆå¤åˆç´¢å¼•å·²è¦†ç›–ï¼‰
DROP INDEX IF EXISTS ix_user_progress_user_id;
DROP INDEX IF EXISTS ix_code_submissions_user_id;
DROP INDEX IF EXISTS ix_chat_messages_user_id;
```

#### æ·»åŠ æ–°ç´¢å¼•
```sql
-- å…¨æ–‡æœç´¢ç´¢å¼•ï¼ˆè¯¾ç¨‹æœç´¢ï¼‰
CREATE INDEX idx_lessons_search ON lessons
USING GIN (to_tsvector('english', title || ' ' || content));

-- èŠå¤©å†…å®¹å…¨æ–‡æœç´¢
CREATE INDEX idx_chat_content_search ON chat_messages
USING GIN (to_tsvector('english', content));

-- éƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•æ´»è·ƒæ•°æ®ï¼‰
CREATE INDEX idx_active_progress ON user_progress(user_id, last_accessed)
WHERE completed = 0;

-- è¡¨è¾¾å¼ç´¢å¼•
CREATE INDEX idx_users_lower_username ON users(LOWER(username));
```

---

## 3. æŸ¥è¯¢æ€§èƒ½åˆ†æ

### 3.1 å¸¸è§æŸ¥è¯¢æ¨¡å¼

#### æŸ¥è¯¢ 1: è·å–ç”¨æˆ·å­¦ä¹ è¿›åº¦ï¼ˆä»ªè¡¨ç›˜ï¼‰
```sql
-- å½“å‰æŸ¥è¯¢
SELECT up.*, l.title, l.chapter_number, l.lesson_number
FROM user_progress up
JOIN lessons l ON up.lesson_id = l.id
WHERE up.user_id = ?
ORDER BY up.last_accessed DESC
LIMIT 10;

-- ç´¢å¼•ä½¿ç”¨: idx_user_last_accessed âœ…
-- æ€§èƒ½: ä¼˜ç§€ï¼ˆ< 10msï¼‰
```

#### æŸ¥è¯¢ 2: è·å–è¯¾ç¨‹æäº¤å†å²
```sql
SELECT *
FROM code_submissions
WHERE user_id = ? AND lesson_id = ?
ORDER BY submitted_at DESC
LIMIT 20;

-- ç´¢å¼•ä½¿ç”¨: idx_submission_user_lesson âœ…
-- æ€§èƒ½: ä¼˜ç§€ï¼ˆ< 10msï¼‰
```

#### æŸ¥è¯¢ 3: è·å–å¯¹è¯å†å²
```sql
SELECT *
FROM chat_messages
WHERE user_id = ? AND lesson_id = ?
ORDER BY created_at DESC
LIMIT 50;

-- ç´¢å¼•ä½¿ç”¨: idx_chat_user_lesson_created âœ…
-- æ€§èƒ½: ä¼˜ç§€ï¼ˆ< 10msï¼‰
```

#### æŸ¥è¯¢ 4: ç»Ÿè®¡è¯¾ç¨‹å®Œæˆç‡
```sql
SELECT lesson_id, COUNT(*) as total,
       SUM(CASE WHEN completed = 1 THEN 1 ELSE 0 END) as completed_count
FROM user_progress
GROUP BY lesson_id;

-- ç´¢å¼•ä½¿ç”¨: idx_lesson_completed âœ…
-- æ€§èƒ½: è‰¯å¥½ï¼ˆ< 50msï¼‰
```

### 3.2 æ½œåœ¨æ…¢æŸ¥è¯¢

#### æ…¢æŸ¥è¯¢ 1: æ— ç´¢å¼•çš„æ—¥æœŸèŒƒå›´æŸ¥è¯¢
```sql
-- âŒ å¯èƒ½æ…¢ï¼ˆæ²¡æœ‰æ—¶é—´èŒƒå›´ç´¢å¼•ï¼‰
SELECT *
FROM code_submissions
WHERE submitted_at >= '2024-01-01' AND submitted_at < '2024-02-01';

-- ä¼˜åŒ–æ–¹æ¡ˆï¼šæ·»åŠ éƒ¨åˆ†ç´¢å¼•
CREATE INDEX idx_submissions_recent ON code_submissions(submitted_at)
WHERE submitted_at >= CURRENT_DATE - INTERVAL '30 days';
```

#### æ…¢æŸ¥è¯¢ 2: å…¨è¡¨æ‰«æçš„èšåˆæŸ¥è¯¢
```sql
-- âŒ å¯èƒ½æ…¢ï¼ˆæ•°æ®é‡å¤§æ—¶ï¼‰
SELECT COUNT(DISTINCT user_id) as active_users
FROM user_progress
WHERE last_accessed >= CURRENT_DATE - INTERVAL '7 days';

-- ä¼˜åŒ–æ–¹æ¡ˆï¼šåˆ›å»ºç‰©åŒ–è§†å›¾
CREATE MATERIALIZED VIEW active_users_stats AS
SELECT DATE(last_accessed) as date, COUNT(DISTINCT user_id) as count
FROM user_progress
WHERE last_accessed >= CURRENT_DATE - INTERVAL '90 days'
GROUP BY DATE(last_accessed);

-- æ¯å°æ—¶åˆ·æ–°
REFRESH MATERIALIZED VIEW CONCURRENTLY active_users_stats;
```

---

## 4. PostgreSQL è¿ç§»æ–¹æ¡ˆ

### 4.1 è¿ç§»ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | åœæœºæ—¶é—´ | é£é™© | å¤æ‚åº¦ | æ¨èåœºæ™¯ |
|------|---------|------|--------|---------|
| **ç›´æ¥è¿ç§»** | 5-10 åˆ†é’Ÿ | ä½ | ç®€å• | âœ… å½“å‰æ¨èï¼ˆæ•°æ®é‡å°ï¼‰ |
| **è“ç»¿éƒ¨ç½²** | 0 åˆ†é’Ÿ | ä¸­ | ä¸­ç­‰ | ç”Ÿäº§ç¯å¢ƒï¼ˆæœªæ¥ï¼‰ |
| **åŒå†™è¿ç§»** | 0 åˆ†é’Ÿ | é«˜ | å¤æ‚ | å¤§è§„æ¨¡è¿ç§» |

### 4.2 æ¨èæ–¹æ¡ˆï¼šç›´æ¥è¿ç§» + å¤‡ä»½å›æ»š

#### é˜¶æ®µ 1: å‡†å¤‡é˜¶æ®µï¼ˆè¿ç§»å‰ 1 å‘¨ï¼‰

```bash
# 1. å®‰è£… PostgreSQL 17
brew install postgresql@17  # macOS
sudo apt install postgresql-17  # Ubuntu

# 2. åˆ›å»ºæ•°æ®åº“å’Œç”¨æˆ·
sudo -u postgres psql
CREATE DATABASE helloagents_prod ENCODING 'UTF8';
CREATE USER helloagents_user WITH PASSWORD 'secure_password_here';
GRANT ALL PRIVILEGES ON DATABASE helloagents_prod TO helloagents_user;

# PostgreSQL 15+ éœ€è¦é¢å¤–æˆæƒ
\c helloagents_prod
GRANT ALL ON SCHEMA public TO helloagents_user;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO helloagents_user;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO helloagents_user;

# 3. é…ç½® PostgreSQL æ€§èƒ½å‚æ•°
# ç¼–è¾‘ /etc/postgresql/17/main/postgresql.conf
shared_buffers = 256MB              # 25% of RAM (å‡è®¾ 1GB RAM)
effective_cache_size = 768MB        # 75% of RAM
maintenance_work_mem = 64MB
work_mem = 10MB
checkpoint_completion_target = 0.9
random_page_cost = 1.1              # SSD ç£ç›˜
effective_io_concurrency = 200      # SSD ç£ç›˜
max_connections = 100

# 4. é‡å¯ PostgreSQL
sudo systemctl restart postgresql
```

#### é˜¶æ®µ 2: æ•°æ®è¿ç§»ï¼ˆåœæœº 5-10 åˆ†é’Ÿï¼‰

```bash
#!/bin/bash
# migrate_to_postgresql.sh

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "ğŸš€ HelloAgents æ•°æ®è¿ç§»å¼€å§‹"
echo "======================================"

# é…ç½®
SQLITE_DB="/path/to/helloagents.db"
PG_HOST="localhost"
PG_PORT="5432"
PG_DB="helloagents_prod"
PG_USER="helloagents_user"
PG_PASSWORD="secure_password_here"
BACKUP_DIR="./backups/$(date +%Y%m%d_%H%M%S)"

# 1. å¤‡ä»½ SQLite æ•°æ®åº“
echo "ğŸ“¦ 1. å¤‡ä»½ SQLite æ•°æ®åº“..."
mkdir -p "$BACKUP_DIR"
cp "$SQLITE_DB" "$BACKUP_DIR/helloagents_backup.db"
sqlite3 "$SQLITE_DB" ".dump" > "$BACKUP_DIR/sqlite_dump.sql"
echo "   âœ… å¤‡ä»½å®Œæˆ: $BACKUP_DIR"

# 2. å¯¼å‡º SQLite æ•°æ®ä¸º CSV
echo "ğŸ“¤ 2. å¯¼å‡º SQLite æ•°æ®..."
sqlite3 "$SQLITE_DB" <<EOF
.headers on
.mode csv
.output $BACKUP_DIR/users.csv
SELECT * FROM users;
.output $BACKUP_DIR/lessons.csv
SELECT * FROM lessons;
.output $BACKUP_DIR/user_progress.csv
SELECT * FROM user_progress;
.output $BACKUP_DIR/code_submissions.csv
SELECT * FROM code_submissions;
.output $BACKUP_DIR/chat_messages.csv
SELECT * FROM chat_messages;
.quit
EOF
echo "   âœ… æ•°æ®å¯¼å‡ºå®Œæˆ"

# 3. åˆ›å»º PostgreSQL è¡¨ç»“æ„
echo "ğŸ”§ 3. åˆ›å»º PostgreSQL è¡¨ç»“æ„..."
export PGPASSWORD="$PG_PASSWORD"
psql -h "$PG_HOST" -p "$PG_PORT" -U "$PG_USER" -d "$PG_DB" -f create_tables.sql
echo "   âœ… è¡¨ç»“æ„åˆ›å»ºå®Œæˆ"

# 4. å¯¼å…¥æ•°æ®åˆ° PostgreSQL
echo "ğŸ“¥ 4. å¯¼å…¥æ•°æ®åˆ° PostgreSQL..."
psql -h "$PG_HOST" -p "$PG_PORT" -U "$PG_USER" -d "$PG_DB" <<EOF
-- ä¸´æ—¶ç¦ç”¨è§¦å‘å™¨å’Œçº¦æŸï¼ˆåŠ é€Ÿå¯¼å…¥ï¼‰
SET session_replication_role = 'replica';

-- å¯¼å…¥æ•°æ®
\COPY users FROM '$BACKUP_DIR/users.csv' WITH CSV HEADER;
\COPY lessons FROM '$BACKUP_DIR/lessons.csv' WITH CSV HEADER;
\COPY user_progress FROM '$BACKUP_DIR/user_progress.csv' WITH CSV HEADER;
\COPY code_submissions FROM '$BACKUP_DIR/code_submissions.csv' WITH CSV HEADER;
\COPY chat_messages FROM '$BACKUP_DIR/chat_messages.csv' WITH CSV HEADER;

-- é‡æ–°å¯ç”¨è§¦å‘å™¨å’Œçº¦æŸ
SET session_replication_role = 'origin';

-- æ›´æ–°åºåˆ—ï¼ˆè‡ªå¢ IDï¼‰
SELECT setval('users_id_seq', (SELECT MAX(id) FROM users));
SELECT setval('lessons_id_seq', (SELECT MAX(id) FROM lessons));
SELECT setval('user_progress_id_seq', (SELECT MAX(id) FROM user_progress));
SELECT setval('code_submissions_id_seq', (SELECT MAX(id) FROM code_submissions));
SELECT setval('chat_messages_id_seq', (SELECT MAX(id) FROM chat_messages));
EOF
echo "   âœ… æ•°æ®å¯¼å…¥å®Œæˆ"

# 5. åˆ›å»ºç´¢å¼•
echo "ğŸ” 5. åˆ›å»ºç´¢å¼•..."
psql -h "$PG_HOST" -p "$PG_PORT" -U "$PG_USER" -d "$PG_DB" -f create_indexes.sql
echo "   âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ"

# 6. æ•°æ®éªŒè¯
echo "âœ… 6. éªŒè¯æ•°æ®å®Œæ•´æ€§..."
psql -h "$PG_HOST" -p "$PG_PORT" -U "$PG_USER" -d "$PG_DB" <<EOF
SELECT 'users' as table_name, COUNT(*) as count FROM users
UNION ALL
SELECT 'lessons', COUNT(*) FROM lessons
UNION ALL
SELECT 'user_progress', COUNT(*) FROM user_progress
UNION ALL
SELECT 'code_submissions', COUNT(*) FROM code_submissions
UNION ALL
SELECT 'chat_messages', COUNT(*) FROM chat_messages;
EOF

echo ""
echo "======================================"
echo "âœ… è¿ç§»å®Œæˆï¼"
echo "======================================"
echo "å¤‡ä»½ä½ç½®: $BACKUP_DIR"
echo "PostgreSQL è¿æ¥: postgresql://$PG_USER:***@$PG_HOST:$PG_PORT/$PG_DB"
```

#### é˜¶æ®µ 3: è¡¨ç»“æ„ä¼˜åŒ–ï¼ˆPostgreSQL ä¸“ç”¨ï¼‰

```sql
-- create_tables_optimized.sql
-- PostgreSQL 17 ä¼˜åŒ–çš„è¡¨ç»“æ„

-- 1. Users è¡¨
CREATE TABLE users (
    id BIGSERIAL PRIMARY KEY,
    username VARCHAR(50) NOT NULL,
    full_name VARCHAR(100),
    settings JSONB DEFAULT '{}',  -- âœ… ä½¿ç”¨ JSONB
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),  -- âœ… ä½¿ç”¨ TIMESTAMPTZ
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    last_login TIMESTAMPTZ,

    CONSTRAINT uk_users_username UNIQUE (username),
    CONSTRAINT chk_username_length CHECK (char_length(username) >= 3)
);

-- è‡ªåŠ¨æ›´æ–° updated_at è§¦å‘å™¨
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER update_users_updated_at
    BEFORE UPDATE ON users
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- 2. Lessons è¡¨
CREATE TABLE lessons (
    id BIGSERIAL PRIMARY KEY,
    chapter_number INTEGER NOT NULL,
    lesson_number INTEGER NOT NULL,
    title VARCHAR(200) NOT NULL,
    content TEXT NOT NULL,  -- TOAST è‡ªåŠ¨å‹ç¼©
    starter_code TEXT,
    extra_data JSONB DEFAULT '{}',  -- âœ… ä½¿ç”¨ JSONB
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    CONSTRAINT uk_chapter_lesson UNIQUE (chapter_number, lesson_number)
);

CREATE TRIGGER update_lessons_updated_at
    BEFORE UPDATE ON lessons
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- 3. UserProgress è¡¨
CREATE TABLE user_progress (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT NOT NULL,
    lesson_id BIGINT NOT NULL,
    completed BOOLEAN DEFAULT FALSE,  -- âœ… ä½¿ç”¨ BOOLEAN
    current_code TEXT,
    cursor_position JSONB DEFAULT '{"line": 1, "column": 1}',  -- âœ… ä½¿ç”¨ JSONB
    started_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    completed_at TIMESTAMPTZ,
    last_accessed TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    CONSTRAINT uk_user_lesson UNIQUE (user_id, lesson_id),
    CONSTRAINT fk_user FOREIGN KEY (user_id)
        REFERENCES users(id) ON DELETE CASCADE,
    CONSTRAINT fk_lesson FOREIGN KEY (lesson_id)
        REFERENCES lessons(id) ON DELETE CASCADE
);

-- 4. CodeSubmission è¡¨
CREATE TABLE code_submissions (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT NOT NULL,
    lesson_id BIGINT NOT NULL,
    code TEXT NOT NULL,
    output TEXT,
    status VARCHAR(20) NOT NULL,
    execution_time DOUBLE PRECISION,
    submitted_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    CONSTRAINT chk_status CHECK (status IN ('success', 'error', 'timeout')),
    CONSTRAINT fk_user FOREIGN KEY (user_id)
        REFERENCES users(id) ON DELETE CASCADE,
    CONSTRAINT fk_lesson FOREIGN KEY (lesson_id)
        REFERENCES lessons(id) ON DELETE CASCADE
);

-- 5. ChatMessage è¡¨
CREATE TABLE chat_messages (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT NOT NULL,
    lesson_id BIGINT,
    role VARCHAR(20) NOT NULL,
    content TEXT NOT NULL,
    extra_data JSONB DEFAULT '{}',  -- âœ… ä½¿ç”¨ JSONB
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    CONSTRAINT chk_role CHECK (role IN ('user', 'assistant', 'system')),
    CONSTRAINT fk_user FOREIGN KEY (user_id)
        REFERENCES users(id) ON DELETE CASCADE,
    CONSTRAINT fk_lesson FOREIGN KEY (lesson_id)
        REFERENCES lessons(id) ON DELETE SET NULL
);

-- è®¾ç½®è¡¨æ³¨é‡Š
COMMENT ON TABLE users IS 'ç”¨æˆ·è´¦æˆ·è¡¨';
COMMENT ON TABLE lessons IS 'è¯¾ç¨‹å†…å®¹è¡¨';
COMMENT ON TABLE user_progress IS 'ç”¨æˆ·å­¦ä¹ è¿›åº¦è¡¨';
COMMENT ON TABLE code_submissions IS 'ä»£ç æäº¤è®°å½•è¡¨';
COMMENT ON TABLE chat_messages IS 'AI å¯¹è¯æ¶ˆæ¯è¡¨';
```

#### é˜¶æ®µ 4: ç´¢å¼•ä¼˜åŒ–

```sql
-- create_indexes_optimized.sql
-- PostgreSQL ä¼˜åŒ–ç´¢å¼•

-- ====================================
-- Users ç´¢å¼•
-- ====================================
CREATE UNIQUE INDEX idx_users_username ON users(username);
CREATE INDEX idx_users_lower_username ON users(LOWER(username));  -- ä¸åŒºåˆ†å¤§å°å†™æŸ¥è¯¢
CREATE INDEX idx_users_last_login ON users(last_login DESC NULLS LAST);  -- æ´»è·ƒç”¨æˆ·æŸ¥è¯¢

-- ====================================
-- Lessons ç´¢å¼•
-- ====================================
CREATE INDEX idx_lessons_chapter ON lessons(chapter_number);
CREATE INDEX idx_lessons_chapter_lesson ON lessons(chapter_number, lesson_number);  -- å¤åˆç´¢å¼•

-- å…¨æ–‡æœç´¢ç´¢å¼•ï¼ˆè¯¾ç¨‹æœç´¢ï¼‰
CREATE INDEX idx_lessons_search ON lessons
USING GIN (to_tsvector('english', title || ' ' || content));

-- JSONB ç´¢å¼•ï¼ˆå…ƒæ•°æ®æŸ¥è¯¢ï¼‰
CREATE INDEX idx_lessons_metadata ON lessons USING GIN (extra_data);

-- ====================================
-- UserProgress ç´¢å¼•
-- ====================================
-- å¤åˆç´¢å¼•ï¼ˆè¦†ç›–ä¸»è¦æŸ¥è¯¢ï¼‰
CREATE INDEX idx_progress_user_completed ON user_progress(user_id, completed);
CREATE INDEX idx_progress_user_accessed ON user_progress(user_id, last_accessed DESC);
CREATE INDEX idx_progress_lesson_completed ON user_progress(lesson_id, completed);
CREATE INDEX idx_progress_user_completed_accessed ON user_progress(user_id, completed, last_accessed DESC);

-- éƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•æ´»è·ƒè¿›åº¦ï¼‰
CREATE INDEX idx_progress_active ON user_progress(user_id, last_accessed DESC)
WHERE completed = FALSE;

-- ====================================
-- CodeSubmission ç´¢å¼•
-- ====================================
-- å¤åˆç´¢å¼•ï¼ˆè¦†ç›–ç»Ÿè®¡æŸ¥è¯¢ï¼‰
CREATE INDEX idx_submission_user_lesson ON code_submissions(user_id, lesson_id);
CREATE INDEX idx_submission_user_submitted ON code_submissions(user_id, submitted_at DESC);
CREATE INDEX idx_submission_lesson_submitted ON code_submissions(lesson_id, submitted_at DESC);
CREATE INDEX idx_submission_lesson_user_status ON code_submissions(lesson_id, user_id, status);

-- éƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•æœ€è¿‘ 30 å¤©çš„æäº¤ï¼‰
CREATE INDEX idx_submission_recent ON code_submissions(submitted_at DESC)
WHERE submitted_at >= CURRENT_DATE - INTERVAL '30 days';

-- ====================================
-- ChatMessage ç´¢å¼•
-- ====================================
-- å¤åˆç´¢å¼•ï¼ˆè¦†ç›–å¯¹è¯æŸ¥è¯¢ï¼‰
CREATE INDEX idx_chat_user_created ON chat_messages(user_id, created_at DESC);
CREATE INDEX idx_chat_user_lesson ON chat_messages(user_id, lesson_id);
CREATE INDEX idx_chat_lesson_created ON chat_messages(lesson_id, created_at DESC);
CREATE INDEX idx_chat_user_lesson_created ON chat_messages(user_id, lesson_id, created_at DESC);

-- å…¨æ–‡æœç´¢ç´¢å¼•ï¼ˆå¯¹è¯æœç´¢ï¼‰
CREATE INDEX idx_chat_content_search ON chat_messages
USING GIN (to_tsvector('english', content));

-- éƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•æœ€è¿‘ 90 å¤©çš„å¯¹è¯ï¼‰
CREATE INDEX idx_chat_recent ON chat_messages(user_id, created_at DESC)
WHERE created_at >= CURRENT_DATE - INTERVAL '90 days';

-- ====================================
-- åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
-- ====================================
ANALYZE users;
ANALYZE lessons;
ANALYZE user_progress;
ANALYZE code_submissions;
ANALYZE chat_messages;
```

#### é˜¶æ®µ 5: åº”ç”¨é…ç½®æ›´æ–°

```python
# backend/app/database.py (æ›´æ–°å)

import os
from pathlib import Path
from sqlalchemy import create_engine, event
from sqlalchemy.orm import sessionmaker, DeclarativeBase
from sqlalchemy.pool import QueuePool
from .logger import get_logger

logger = get_logger(__name__)

# æ•°æ®åº“ URLï¼ˆä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
DATABASE_URL = os.environ.get('DATABASE_URL')

if not DATABASE_URL:
    # å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨ SQLite
    BASE_DIR = Path(__file__).resolve().parent.parent
    DATABASE_PATH = BASE_DIR / 'helloagents.db'
    DATABASE_URL = f'sqlite:///{DATABASE_PATH}'
    IS_POSTGRES = False
else:
    # ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨ PostgreSQL
    IS_POSTGRES = DATABASE_URL.startswith('postgresql')
    DATABASE_PATH = None

# PostgreSQL è¿æ¥æ± é…ç½®
if IS_POSTGRES:
    engine = create_engine(
        DATABASE_URL,
        poolclass=QueuePool,
        pool_size=20,              # åŸºç¡€è¿æ¥æ•°
        max_overflow=40,           # æœ€å¤§é¢å¤–è¿æ¥æ•°
        pool_recycle=3600,         # 1å°æ—¶å›æ”¶è¿æ¥
        pool_pre_ping=True,        # è¿æ¥å¥åº·æ£€æŸ¥
        pool_timeout=30,           # è¿æ¥è¶…æ—¶ 30 ç§’
        echo=False,                # ç”Ÿäº§ç¯å¢ƒå…³é—­ SQL æ—¥å¿—
        connect_args={
            'connect_timeout': 10,
            'options': '-c timezone=utc',  # å¼ºåˆ¶ UTC æ—¶åŒº
        }
    )
    logger.info(
        "postgresql_engine_initialized",
        pool_size=20,
        max_overflow=40
    )
else:
    # SQLite é…ç½®ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
    engine = create_engine(
        DATABASE_URL,
        connect_args={'check_same_thread': False, 'timeout': 30},
        poolclass=StaticPool,
        echo=False,
    )

    # SQLite æ€§èƒ½ä¼˜åŒ–
    @event.listens_for(engine, "connect")
    def set_sqlite_pragma(dbapi_conn, connection_record):
        cursor = dbapi_conn.cursor()
        cursor.execute("PRAGMA foreign_keys = ON")
        cursor.execute("PRAGMA journal_mode = WAL")
        cursor.execute("PRAGMA synchronous = NORMAL")
        cursor.execute("PRAGMA cache_size = -128000")
        cursor.execute("PRAGMA temp_store = MEMORY")
        cursor.execute("PRAGMA mmap_size = 268435456")
        cursor.close()

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

class Base(DeclarativeBase):
    pass

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

```bash
# .env (ç”Ÿäº§ç¯å¢ƒé…ç½®)
DATABASE_URL=postgresql://helloagents_user:secure_password@localhost:5432/helloagents_prod
DEEPSEEK_API_KEY=your_api_key_here
DEBUG=false
LOG_SQL_QUERIES=false

# Sentry ç›‘æ§ï¼ˆå¯é€‰ï¼‰
SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id
```

#### é˜¶æ®µ 6: éªŒè¯å’Œæµ‹è¯•

```bash
# 1. è¿è¡Œå•å…ƒæµ‹è¯•
cd backend
pytest tests/ -v

# 2. è¿è¡Œé›†æˆæµ‹è¯•
pytest tests/integration/ -v

# 3. æ€§èƒ½æµ‹è¯•ï¼ˆå¯¹æ¯” SQLite vs PostgreSQLï¼‰
python tests/performance/benchmark_queries.py

# 4. æ•°æ®ä¸€è‡´æ€§éªŒè¯
python scripts/validate_migration.py
```

### 4.3 å›æ»šæ–¹æ¡ˆ

```bash
#!/bin/bash
# rollback_migration.sh

set -e

echo "ğŸ”„ å›æ»šæ•°æ®åº“è¿ç§»"
echo "======================================"

BACKUP_DIR="./backups/æœ€æ–°å¤‡ä»½ç›®å½•"
SQLITE_DB="/path/to/helloagents.db"

# 1. åœæ­¢åº”ç”¨
echo "â¸ï¸  åœæ­¢åº”ç”¨..."
sudo systemctl stop helloagents

# 2. æ¢å¤ SQLite æ•°æ®åº“
echo "ğŸ“¦ æ¢å¤ SQLite æ•°æ®åº“..."
cp "$BACKUP_DIR/helloagents_backup.db" "$SQLITE_DB"

# 3. æ›´æ–°ç¯å¢ƒå˜é‡ï¼ˆç§»é™¤ DATABASE_URLï¼‰
echo "ğŸ”§ æ¢å¤é…ç½®..."
sed -i '/DATABASE_URL=/d' /path/to/.env

# 4. é‡å¯åº”ç”¨
echo "ğŸš€ é‡å¯åº”ç”¨..."
sudo systemctl start helloagents

echo "âœ… å›æ»šå®Œæˆï¼"
```

---

## 5. æ•°æ®å¤‡ä»½ç­–ç•¥

### 5.1 å¤‡ä»½æ–¹æ¡ˆè®¾è®¡

| å¤‡ä»½ç±»å‹ | é¢‘ç‡ | ä¿ç•™æœŸ | å­˜å‚¨ä½ç½® | æ¢å¤æ—¶é—´ |
|---------|------|-------|---------|---------|
| **å…¨é‡å¤‡ä»½** | æ¯å¤© 03:00 | 30 å¤© | AWS S3 / æœ¬åœ° NAS | 10-30 åˆ†é’Ÿ |
| **å¢é‡å¤‡ä»½** | æ¯å°æ—¶ | 7 å¤© | æœ¬åœ°ç£ç›˜ | 5-10 åˆ†é’Ÿ |
| **WAL å½’æ¡£** | å®æ—¶ | 7 å¤© | æœ¬åœ°ç£ç›˜ | 1-5 åˆ†é’Ÿ (PITR) |
| **é€»è¾‘å¤‡ä»½** | æ¯å‘¨ | 90 å¤© | AWS S3 | 30-60 åˆ†é’Ÿ |

### 5.2 PostgreSQL å¤‡ä»½è„šæœ¬

#### å…¨é‡å¤‡ä»½ï¼ˆpg_dumpï¼‰

```bash
#!/bin/bash
# backup_postgresql.sh

set -e

# é…ç½®
PG_HOST="localhost"
PG_PORT="5432"
PG_DB="helloagents_prod"
PG_USER="helloagents_user"
BACKUP_DIR="/var/backups/postgresql"
S3_BUCKET="s3://your-backup-bucket/helloagents"
RETENTION_DAYS=30

# åˆ›å»ºå¤‡ä»½ç›®å½•
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="$BACKUP_DIR/$DATE"
mkdir -p "$BACKUP_PATH"

echo "ğŸš€ PostgreSQL å…¨é‡å¤‡ä»½å¼€å§‹"
echo "======================================"

# 1. å…¨é‡å¤‡ä»½ï¼ˆè‡ªå®šä¹‰æ ¼å¼ï¼Œæ”¯æŒå¹¶è¡Œæ¢å¤ï¼‰
echo "ğŸ“¦ 1. åˆ›å»ºå…¨é‡å¤‡ä»½..."
export PGPASSWORD="$PG_PASSWORD"
pg_dump -h "$PG_HOST" -p "$PG_PORT" -U "$PG_USER" -d "$PG_DB" \
    -Fc \
    -Z 9 \
    -f "$BACKUP_PATH/full_backup.dump"

echo "   âœ… å…¨é‡å¤‡ä»½å®Œæˆ: $(du -h $BACKUP_PATH/full_backup.dump | cut -f1)"

# 2. å¯¼å‡º SQL è„šæœ¬ï¼ˆä¾¿äºæŸ¥çœ‹å’Œæ‰‹åŠ¨æ¢å¤ï¼‰
echo "ğŸ“¤ 2. å¯¼å‡º SQL è„šæœ¬..."
pg_dump -h "$PG_HOST" -p "$PG_PORT" -U "$PG_USER" -d "$PG_DB" \
    --clean --if-exists \
    -f "$BACKUP_PATH/schema_and_data.sql"

gzip "$BACKUP_PATH/schema_and_data.sql"
echo "   âœ… SQL è„šæœ¬å¯¼å‡ºå®Œæˆ"

# 3. ä»…å¯¼å‡ºè¡¨ç»“æ„ï¼ˆä¾¿äºå¿«é€ŸæŸ¥çœ‹ï¼‰
echo "ğŸ”§ 3. å¯¼å‡ºè¡¨ç»“æ„..."
pg_dump -h "$PG_HOST" -p "$PG_PORT" -U "$PG_USER" -d "$PG_DB" \
    --schema-only \
    -f "$BACKUP_PATH/schema_only.sql"

echo "   âœ… è¡¨ç»“æ„å¯¼å‡ºå®Œæˆ"

# 4. å¯¼å‡ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
echo "ğŸ“Š 4. å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯..."
psql -h "$PG_HOST" -p "$PG_PORT" -U "$PG_USER" -d "$PG_DB" <<EOF > "$BACKUP_PATH/stats.txt"
SELECT 'users' as table_name, COUNT(*) as count FROM users
UNION ALL SELECT 'lessons', COUNT(*) FROM lessons
UNION ALL SELECT 'user_progress', COUNT(*) FROM user_progress
UNION ALL SELECT 'code_submissions', COUNT(*) FROM code_submissions
UNION ALL SELECT 'chat_messages', COUNT(*) FROM chat_messages;

SELECT pg_size_pretty(pg_database_size('$PG_DB')) as database_size;
EOF

cat "$BACKUP_PATH/stats.txt"

# 5. ä¸Šä¼ åˆ° S3ï¼ˆå¯é€‰ï¼‰
if command -v aws &> /dev/null; then
    echo "â˜ï¸  5. ä¸Šä¼ åˆ° S3..."
    aws s3 sync "$BACKUP_PATH" "$S3_BUCKET/$DATE/" --quiet
    echo "   âœ… S3 ä¸Šä¼ å®Œæˆ"
fi

# 6. æ¸…ç†æ—§å¤‡ä»½
echo "ğŸ§¹ 6. æ¸…ç†æ—§å¤‡ä»½..."
find "$BACKUP_DIR" -type d -mtime +$RETENTION_DAYS -exec rm -rf {} + 2>/dev/null || true
echo "   âœ… æ—§å¤‡ä»½å·²æ¸…ç†ï¼ˆä¿ç•™ $RETENTION_DAYS å¤©ï¼‰"

echo ""
echo "======================================"
echo "âœ… å¤‡ä»½å®Œæˆï¼"
echo "======================================"
echo "å¤‡ä»½ä½ç½®: $BACKUP_PATH"
echo "å¤‡ä»½å¤§å°: $(du -sh $BACKUP_PATH | cut -f1)"
```

#### WAL å½’æ¡£é…ç½®

```ini
# postgresql.conf

# å¯ç”¨ WAL å½’æ¡£
wal_level = replica
archive_mode = on
archive_command = 'test ! -f /var/lib/postgresql/wal_archive/%f && cp %p /var/lib/postgresql/wal_archive/%f'
archive_timeout = 300  # 5 åˆ†é’Ÿ

# WAL ä¿ç•™
wal_keep_size = 1GB
max_wal_senders = 3
```

#### æ—¶é—´ç‚¹æ¢å¤ (PITR)

```bash
#!/bin/bash
# restore_pitr.sh
# æ¢å¤åˆ°æŒ‡å®šæ—¶é—´ç‚¹

set -e

TARGET_TIME="2026-01-10 14:30:00"
BACKUP_FILE="/var/backups/postgresql/20260110_030000/full_backup.dump"
WAL_ARCHIVE="/var/lib/postgresql/wal_archive"

echo "ğŸ”„ æ—¶é—´ç‚¹æ¢å¤ (PITR)"
echo "ç›®æ ‡æ—¶é—´: $TARGET_TIME"
echo "======================================"

# 1. åœæ­¢ PostgreSQL
sudo systemctl stop postgresql

# 2. æ¸…ç©ºæ•°æ®ç›®å½•
sudo rm -rf /var/lib/postgresql/17/main/*

# 3. æ¢å¤åŸºç¡€å¤‡ä»½
sudo -u postgres pg_restore -d postgres -C "$BACKUP_FILE"

# 4. åˆ›å»ºæ¢å¤é…ç½®
sudo -u postgres cat > /var/lib/postgresql/17/main/recovery.conf <<EOF
restore_command = 'cp $WAL_ARCHIVE/%f %p'
recovery_target_time = '$TARGET_TIME'
recovery_target_action = 'promote'
EOF

# 5. å¯åŠ¨ PostgreSQLï¼ˆè‡ªåŠ¨æ‰§è¡Œ PITRï¼‰
sudo systemctl start postgresql

echo "âœ… PITR æ¢å¤å®Œæˆï¼"
```

### 5.3 SQLite å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# backup_sqlite.sh

set -e

SQLITE_DB="/path/to/helloagents.db"
BACKUP_DIR="/var/backups/sqlite"
RETENTION_DAYS=30

DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="$BACKUP_DIR/$DATE"
mkdir -p "$BACKUP_PATH"

echo "ğŸš€ SQLite å¤‡ä»½å¼€å§‹"

# 1. æ–‡ä»¶å¤‡ä»½ï¼ˆä½¿ç”¨ .backup å‘½ä»¤ï¼Œåœ¨çº¿å¤‡ä»½ï¼‰
sqlite3 "$SQLITE_DB" ".backup '$BACKUP_PATH/helloagents.db'"

# 2. å¯¼å‡º SQL è„šæœ¬
sqlite3 "$SQLITE_DB" ".dump" | gzip > "$BACKUP_PATH/dump.sql.gz"

# 3. å¯¼å‡º CSVï¼ˆå¯é€‰ï¼‰
sqlite3 "$SQLITE_DB" <<EOF
.headers on
.mode csv
.output $BACKUP_PATH/users.csv
SELECT * FROM users;
.output $BACKUP_PATH/lessons.csv
SELECT * FROM lessons;
.quit
EOF

# 4. æ¸…ç†æ—§å¤‡ä»½
find "$BACKUP_DIR" -type d -mtime +$RETENTION_DAYS -exec rm -rf {} + 2>/dev/null || true

echo "âœ… SQLite å¤‡ä»½å®Œæˆ: $BACKUP_PATH"
```

---

## 6. æ•°æ®åº“æ‰©å±•æ–¹æ¡ˆ

### 6.1 çŸ­æœŸæ‰©å±•ï¼ˆ1-10 ä¸‡ç”¨æˆ·ï¼‰

#### å‚ç›´æ‰©å±•ï¼ˆScale Upï¼‰
```yaml
# æ¨èæœåŠ¡å™¨é…ç½®
CPU: 4-8 æ ¸
RAM: 8-16 GB
ç£ç›˜: 100 GB SSD (NVMe)
ç½‘ç»œ: 1 Gbps

# PostgreSQL é…ç½®
shared_buffers: 2-4 GB
effective_cache_size: 6-12 GB
max_connections: 200
work_mem: 20 MB
```

#### è¿æ¥æ± ä¼˜åŒ–
```python
# backend/app/database.py

# PgBouncer è¿æ¥æ± é…ç½®
DATABASE_URL = "postgresql://user:pass@localhost:6432/db"  # PgBouncer ç«¯å£

engine = create_engine(
    DATABASE_URL,
    pool_size=20,
    max_overflow=40,
    pool_pre_ping=True,
)
```

```ini
# /etc/pgbouncer/pgbouncer.ini

[databases]
helloagents_prod = host=localhost port=5432 dbname=helloagents_prod

[pgbouncer]
listen_addr = *
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 20
reserve_pool_size = 5
reserve_pool_timeout = 3
```

### 6.2 ä¸­æœŸæ‰©å±•ï¼ˆ10-100 ä¸‡ç”¨æˆ·ï¼‰

#### è¯»å†™åˆ†ç¦»
```yaml
# æ¶æ„è®¾è®¡
ä¸»åº“ï¼ˆMasterï¼‰: å¤„ç†æ‰€æœ‰å†™æ“ä½œ
ä»åº“ 1ï¼ˆReplica 1ï¼‰: å¤„ç†è¯»æ“ä½œï¼ˆè¯¾ç¨‹å†…å®¹ã€ç”¨æˆ·è¿›åº¦ï¼‰
ä»åº“ 2ï¼ˆReplica 2ï¼‰: å¤„ç†è¯»æ“ä½œï¼ˆå¯¹è¯å†å²ã€ä»£ç æäº¤ï¼‰
```

```python
# backend/app/database_multi.py
# è¯»å†™åˆ†ç¦»é…ç½®

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# ä¸»åº“ï¼ˆå†™ï¼‰
master_engine = create_engine(
    "postgresql://user:pass@master-db:5432/db",
    pool_size=20,
    max_overflow=40,
)

# ä»åº“ 1ï¼ˆè¯»ï¼‰
replica1_engine = create_engine(
    "postgresql://user:pass@replica1-db:5432/db",
    pool_size=30,
    max_overflow=60,
)

# ä»åº“ 2ï¼ˆè¯»ï¼‰
replica2_engine = create_engine(
    "postgresql://user:pass@replica2-db:5432/db",
    pool_size=30,
    max_overflow=60,
)

# Session å·¥å‚
MasterSession = sessionmaker(bind=master_engine)
ReplicaSession1 = sessionmaker(bind=replica1_engine)
ReplicaSession2 = sessionmaker(bind=replica2_engine)

def get_db_master():
    """è·å–ä¸»åº“ä¼šè¯ï¼ˆå†™æ“ä½œï¼‰"""
    db = MasterSession()
    try:
        yield db
    finally:
        db.close()

def get_db_replica_lessons():
    """è·å–ä»åº“ä¼šè¯ï¼ˆè¯¾ç¨‹æŸ¥è¯¢ï¼‰"""
    db = ReplicaSession1()
    try:
        yield db
    finally:
        db.close()

def get_db_replica_chat():
    """è·å–ä»åº“ä¼šè¯ï¼ˆå¯¹è¯æŸ¥è¯¢ï¼‰"""
    db = ReplicaSession2()
    try:
        yield db
    finally:
        db.close()
```

#### åˆ†åŒºè¡¨
```sql
-- æŒ‰æ—¶é—´åˆ†åŒº chat_messages è¡¨ï¼ˆæŒ‰æœˆåˆ†åŒºï¼‰

-- 1. åˆ›å»ºåˆ†åŒºä¸»è¡¨
CREATE TABLE chat_messages (
    id BIGSERIAL,
    user_id BIGINT NOT NULL,
    lesson_id BIGINT,
    role VARCHAR(20) NOT NULL,
    content TEXT NOT NULL,
    extra_data JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
) PARTITION BY RANGE (created_at);

-- 2. åˆ›å»ºåˆ†åŒº
CREATE TABLE chat_messages_2026_01 PARTITION OF chat_messages
    FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');

CREATE TABLE chat_messages_2026_02 PARTITION OF chat_messages
    FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');

-- ... ç»§ç»­åˆ›å»ºåç»­æœˆä»½åˆ†åŒº

-- 3. è‡ªåŠ¨åˆ›å»ºåˆ†åŒºå‡½æ•°
CREATE OR REPLACE FUNCTION create_monthly_partition(
    table_name TEXT,
    start_date DATE
) RETURNS VOID AS $$
DECLARE
    partition_name TEXT;
    start_ts TIMESTAMPTZ;
    end_ts TIMESTAMPTZ;
BEGIN
    partition_name := table_name || '_' || TO_CHAR(start_date, 'YYYY_MM');
    start_ts := start_date;
    end_ts := start_date + INTERVAL '1 month';

    EXECUTE format(
        'CREATE TABLE IF NOT EXISTS %I PARTITION OF %I
         FOR VALUES FROM (%L) TO (%L)',
        partition_name, table_name, start_ts, end_ts
    );

    RAISE NOTICE 'Created partition: %', partition_name;
END;
$$ LANGUAGE plpgsql;

-- 4. è‡ªåŠ¨åˆ›å»ºæœªæ¥ 3 ä¸ªæœˆçš„åˆ†åŒºï¼ˆå®šæ—¶ä»»åŠ¡ï¼‰
DO $$
DECLARE
    i INTEGER;
BEGIN
    FOR i IN 0..2 LOOP
        PERFORM create_monthly_partition(
            'chat_messages',
            DATE_TRUNC('month', CURRENT_DATE + (i || ' months')::INTERVAL)::DATE
        );
    END LOOP;
END $$;
```

### 6.3 é•¿æœŸæ‰©å±•ï¼ˆ100 ä¸‡+ ç”¨æˆ·ï¼‰

#### æ°´å¹³åˆ†ç‰‡ï¼ˆShardingï¼‰
```yaml
# åˆ†ç‰‡ç­–ç•¥ï¼šæŒ‰ç”¨æˆ· ID åˆ†ç‰‡

# Shard 1: user_id % 4 = 0
database: helloagents_shard1
users: 25%

# Shard 2: user_id % 4 = 1
database: helloagents_shard2
users: 25%

# Shard 3: user_id % 4 = 2
database: helloagents_shard3
users: 25%

# Shard 4: user_id % 4 = 3
database: helloagents_shard4
users: 25%
```

```python
# backend/app/database_sharding.py

class ShardRouter:
    """åˆ†ç‰‡è·¯ç”±å™¨"""

    def __init__(self):
        self.shards = {
            0: create_engine("postgresql://user:pass@shard1:5432/db"),
            1: create_engine("postgresql://user:pass@shard2:5432/db"),
            2: create_engine("postgresql://user:pass@shard3:5432/db"),
            3: create_engine("postgresql://user:pass@shard4:5432/db"),
        }

    def get_shard(self, user_id: int):
        """æ ¹æ® user_id è·å–åˆ†ç‰‡"""
        shard_id = user_id % 4
        return self.shards[shard_id]

    def get_session(self, user_id: int):
        """è·å–ç”¨æˆ·å¯¹åº”åˆ†ç‰‡çš„ä¼šè¯"""
        engine = self.get_shard(user_id)
        Session = sessionmaker(bind=engine)
        return Session()

# ä½¿ç”¨ç¤ºä¾‹
router = ShardRouter()

def get_user_progress(user_id: int, lesson_id: int):
    session = router.get_session(user_id)
    progress = session.query(UserProgress).filter(
        UserProgress.user_id == user_id,
        UserProgress.lesson_id == lesson_id
    ).first()
    session.close()
    return progress
```

#### ç¼“å­˜å±‚ï¼ˆRedisï¼‰
```python
# backend/app/cache.py

import redis
import json
from functools import wraps

# Redis è¿æ¥
redis_client = redis.Redis(
    host='localhost',
    port=6379,
    db=0,
    decode_responses=True,
    socket_connect_timeout=5,
    socket_timeout=5,
)

def cache_query(ttl=3600):
    """æŸ¥è¯¢ç»“æœç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{func.__name__}:{args}:{kwargs}"

            # å°è¯•ä»ç¼“å­˜è·å–
            cached = redis_client.get(cache_key)
            if cached:
                return json.loads(cached)

            # ç¼“å­˜æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢æ•°æ®åº“
            result = await func(*args, **kwargs)

            # å†™å…¥ç¼“å­˜
            redis_client.setex(
                cache_key,
                ttl,
                json.dumps(result)
            )

            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@cache_query(ttl=1800)  # ç¼“å­˜ 30 åˆ†é’Ÿ
async def get_lesson_content(lesson_id: int):
    """è·å–è¯¾ç¨‹å†…å®¹ï¼ˆé«˜é¢‘è¯»å–ï¼Œé€‚åˆç¼“å­˜ï¼‰"""
    db = SessionLocal()
    lesson = db.query(Lesson).filter(Lesson.id == lesson_id).first()
    db.close()
    return lesson.to_dict() if lesson else None
```

---

## 7. ç›‘æ§å’Œå‘Šè­¦

### 7.1 æ€§èƒ½ç›‘æ§æŒ‡æ ‡

```sql
-- PostgreSQL æ€§èƒ½ç›‘æ§æŸ¥è¯¢

-- 1. æ…¢æŸ¥è¯¢ç»Ÿè®¡
SELECT
    query,
    calls,
    total_exec_time / 1000 as total_seconds,
    mean_exec_time / 1000 as avg_seconds,
    max_exec_time / 1000 as max_seconds
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 20;

-- 2. è¡¨å¤§å°ç»Ÿè®¡
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- 3. ç´¢å¼•ä½¿ç”¨ç‡
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan as scans,
    pg_size_pretty(pg_relation_size(indexrelid)) as size
FROM pg_stat_user_indexes
ORDER BY idx_scan ASC;

-- 4. ç¼“å­˜å‘½ä¸­ç‡
SELECT
    sum(heap_blks_read) as heap_read,
    sum(heap_blks_hit)  as heap_hit,
    sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) * 100 as cache_hit_ratio
FROM pg_statio_user_tables;

-- 5. è¿æ¥æ•°ç»Ÿè®¡
SELECT
    state,
    COUNT(*) as connections
FROM pg_stat_activity
WHERE datname = 'helloagents_prod'
GROUP BY state;
```

### 7.2 å‘Šè­¦è§„åˆ™

```yaml
# Prometheus å‘Šè­¦è§„åˆ™

groups:
  - name: postgresql
    rules:
      # æ…¢æŸ¥è¯¢å‘Šè­¦
      - alert: SlowQuery
        expr: pg_stat_statements_mean_exec_time_seconds > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "æ…¢æŸ¥è¯¢æ£€æµ‹ (å®ä¾‹ {{ $labels.instance }})"
          description: "å¹³å‡æŸ¥è¯¢æ—¶é—´è¶…è¿‡ 1 ç§’"

      # ç¼“å­˜å‘½ä¸­ç‡å‘Šè­¦
      - alert: LowCacheHitRate
        expr: pg_cache_hit_ratio < 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½ (å®ä¾‹ {{ $labels.instance }})"
          description: "ç¼“å­˜å‘½ä¸­ç‡ {{ $value }}% < 90%"

      # è¿æ¥æ•°å‘Šè­¦
      - alert: HighConnectionCount
        expr: pg_stat_activity_count > 180
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "è¿æ¥æ•°è¿‡é«˜ (å®ä¾‹ {{ $labels.instance }})"
          description: "å½“å‰è¿æ¥æ•° {{ $value }} > 180"

      # ç£ç›˜ç©ºé—´å‘Šè­¦
      - alert: DiskSpaceUsage
        expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes > 0.8
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "ç£ç›˜ç©ºé—´ä¸è¶³ (å®ä¾‹ {{ $labels.instance }})"
          description: "ç£ç›˜ä½¿ç”¨ç‡ {{ $value }}% > 80%"
```

---

## 8. è¿ç§»æ—¶é—´è¡¨

### 8.1 æ¨èæ—¶é—´è¡¨ï¼ˆ2 å‘¨è®¡åˆ’ï¼‰

| é˜¶æ®µ | æ—¶é—´ | ä»»åŠ¡ | è´Ÿè´£äºº |
|------|------|------|--------|
| **å‡†å¤‡é˜¶æ®µ** | Week 1 Day 1-2 | 1. å®‰è£… PostgreSQL 17<br>2. é…ç½®æ€§èƒ½å‚æ•°<br>3. åˆ›å»ºæ•°æ®åº“å’Œç”¨æˆ· | DevOps / SRE |
| **æµ‹è¯•é˜¶æ®µ** | Week 1 Day 3-5 | 1. ç¼–å†™è¿ç§»è„šæœ¬<br>2. åœ¨æµ‹è¯•ç¯å¢ƒæ‰§è¡Œè¿ç§»<br>3. éªŒè¯æ•°æ®å®Œæ•´æ€§<br>4. æ€§èƒ½æµ‹è¯• | DB Architect / Backend Lead |
| **ä¼˜åŒ–é˜¶æ®µ** | Week 2 Day 1-2 | 1. ä¼˜åŒ–ç´¢å¼•<br>2. åˆ›å»ºåˆ†åŒºè¡¨<br>3. é…ç½®å¤‡ä»½ç­–ç•¥ | DB Architect |
| **ä¸Šçº¿å‡†å¤‡** | Week 2 Day 3-4 | 1. ç¼–å†™å›æ»šæ–¹æ¡ˆ<br>2. å‡†å¤‡ç›‘æ§å‘Šè­¦<br>3. å›¢é˜ŸåŸ¹è®­ | å…¨å›¢é˜Ÿ |
| **æ‰§è¡Œè¿ç§»** | Week 2 Day 5 | 1. å‡Œæ™¨ 3:00 æ‰§è¡Œè¿ç§»<br>2. éªŒè¯æ•°æ®<br>3. æ€§èƒ½ç›‘æ§ | å…¨å›¢é˜Ÿ |
| **ç›‘æ§è§‚å¯Ÿ** | Week 3 | 1. 7x24 ç›‘æ§<br>2. æ€§èƒ½è°ƒä¼˜<br>3. é—®é¢˜ä¿®å¤ | SRE / DB Architect |

### 8.2 è¿ç§»æ£€æŸ¥æ¸…å•

#### è¿ç§»å‰æ£€æŸ¥
- [ ] PostgreSQL 17 å®‰è£…å®Œæˆ
- [ ] æ•°æ®åº“å’Œç”¨æˆ·åˆ›å»ºå®Œæˆ
- [ ] è¿ç§»è„šæœ¬æµ‹è¯•é€šè¿‡
- [ ] å¤‡ä»½ç­–ç•¥é…ç½®å®Œæˆ
- [ ] å›æ»šæ–¹æ¡ˆå‡†å¤‡å®Œæ¯•
- [ ] ç›‘æ§å‘Šè­¦é…ç½®å®Œæˆ
- [ ] å›¢é˜Ÿæˆå‘˜åŸ¹è®­å®Œæˆ
- [ ] ç”¨æˆ·é€šçŸ¥å·²å‘é€

#### è¿ç§»ä¸­æ£€æŸ¥
- [ ] SQLite æ•°æ®å¤‡ä»½å®Œæˆ
- [ ] æ•°æ®å¯¼å‡ºæˆåŠŸ
- [ ] PostgreSQL è¡¨ç»“æ„åˆ›å»ºå®Œæˆ
- [ ] æ•°æ®å¯¼å…¥æˆåŠŸ
- [ ] ç´¢å¼•åˆ›å»ºå®Œæˆ
- [ ] æ•°æ®éªŒè¯é€šè¿‡
- [ ] åº”ç”¨é…ç½®æ›´æ–°å®Œæˆ
- [ ] åº”ç”¨å¯åŠ¨æˆåŠŸ

#### è¿ç§»åæ£€æŸ¥
- [ ] æ‰€æœ‰ API ç«¯ç‚¹æ­£å¸¸å“åº”
- [ ] æ•°æ®æŸ¥è¯¢æ€§èƒ½æ­£å¸¸
- [ ] ç›‘æ§æŒ‡æ ‡æ­£å¸¸
- [ ] æ— å¼‚å¸¸æ—¥å¿—
- [ ] ç”¨æˆ·åé¦ˆæ­£å¸¸
- [ ] å¤‡ä»½ä»»åŠ¡æ­£å¸¸è¿è¡Œ

---

## 9. æˆæœ¬ä¼°ç®—

### 9.1 æœåŠ¡å™¨æˆæœ¬ï¼ˆæŒ‰æœˆè®¡ç®—ï¼‰

| ç¯å¢ƒ | é…ç½® | æ•°æ®åº“ | æœˆæˆæœ¬ | å¤‡æ³¨ |
|------|------|--------|--------|------|
| **å¼€å‘ç¯å¢ƒ** | æœ¬åœ°/SQLite | SQLite | $0 | å…è´¹ |
| **æµ‹è¯•ç¯å¢ƒ** | 2 æ ¸ 4GB | PostgreSQL | $20 | AWS RDS t3.small |
| **ç”Ÿäº§ç¯å¢ƒï¼ˆå°å‹ï¼‰** | 4 æ ¸ 8GB | PostgreSQL | $120 | AWS RDS t3.large |
| **ç”Ÿäº§ç¯å¢ƒï¼ˆä¸­å‹ï¼‰** | 8 æ ¸ 16GB | PostgreSQL | $300 | AWS RDS m5.2xlarge |
| **ç”Ÿäº§ç¯å¢ƒï¼ˆå¤§å‹ï¼‰** | 16 æ ¸ 32GB + ä¸»ä» | PostgreSQL | $800 | AWS RDS r5.4xlarge + å‰¯æœ¬ |

### 9.2 è¿ç§»æˆæœ¬

| é¡¹ç›® | æ—¶é—´ | æˆæœ¬ | å¤‡æ³¨ |
|------|------|------|------|
| **äººåŠ›æˆæœ¬** | 80 å°æ—¶ | $8,000 | DB Architect + DevOps + Backend Lead |
| **æµ‹è¯•ç¯å¢ƒ** | 2 å‘¨ | $40 | AWS RDS æµ‹è¯•å®ä¾‹ |
| **å¤‡ä»½å­˜å‚¨** | æŒç»­ | $10/æœˆ | AWS S3 |
| **ç›‘æ§å·¥å…·** | æŒç»­ | $30/æœˆ | DataDog / New Relic |
| **åˆè®¡** | - | ~$8,080 + $40/æœˆ | ä¸€æ¬¡æ€§ + æŒç»­æˆæœ¬ |

---

## 10. é£é™©è¯„ä¼°

### 10.1 æŠ€æœ¯é£é™©

| é£é™© | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£æªæ–½ |
|------|--------|------|---------|
| **æ•°æ®ä¸¢å¤±** | ä½ | é«˜ | å¤šé‡å¤‡ä»½ + éªŒè¯è„šæœ¬ |
| **è¿ç§»å¤±è´¥** | ä½ | ä¸­ | å®Œå–„å›æ»šæ–¹æ¡ˆ + æµ‹è¯•éªŒè¯ |
| **æ€§èƒ½ä¸‹é™** | ä½ | ä¸­ | æ€§èƒ½æµ‹è¯• + ç´¢å¼•ä¼˜åŒ– |
| **åœæœºæ—¶é—´è¿‡é•¿** | ä½ | ä¸­ | é¢„æ¼”è¿ç§» + è‡ªåŠ¨åŒ–è„šæœ¬ |
| **åº”ç”¨å…¼å®¹æ€§é—®é¢˜** | ä¸­ | ä½ | å……åˆ†æµ‹è¯• + ä»£ç å®¡æŸ¥ |

### 10.2 ä¸šåŠ¡é£é™©

| é£é™© | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£æªæ–½ |
|------|--------|------|---------|
| **ç”¨æˆ·ä½“éªŒä¸‹é™** | ä½ | ä¸­ | å‡Œæ™¨è¿ç§» + ç”¨æˆ·é€šçŸ¥ |
| **æ•°æ®ä¸ä¸€è‡´** | ä½ | é«˜ | äº‹åŠ¡ä¿è¯ + ä¸€è‡´æ€§éªŒè¯ |
| **æˆæœ¬è¶…æ”¯** | ä¸­ | ä½ | æˆæœ¬é¢„ç®— + åˆ†é˜¶æ®µå®æ–½ |

---

## 11. ç»“è®ºä¸å»ºè®®

### 11.1 æ ¸å¿ƒç»“è®º

1. **å½“å‰æ¶æ„è¯„ä¼°**: ğŸŸ¢ è‰¯å¥½
   - SQLite ä¼˜åŒ–è‰¯å¥½ï¼Œé€‚åˆå¼€å‘ç¯å¢ƒ
   - æ•°æ®æ¨¡å‹è®¾è®¡åˆç†ï¼Œç´¢å¼•è¦†ç›–å®Œå–„
   - å·²æ”¯æŒ PostgreSQL é…ç½®ï¼Œè¿ç§»å‡†å¤‡å……åˆ†

2. **è¿ç§»å¿…è¦æ€§**: ğŸŸ¢ æ¨èè¿ç§»
   - ç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨ PostgreSQLï¼ˆé«˜å¹¶å‘ã€æ•°æ®å®‰å…¨ï¼‰
   - å½“å‰æ•°æ®é‡å°ï¼ˆ1.3 MBï¼‰ï¼Œè¿ç§»é£é™©ä½
   - ç´¢å¼•å’ŒæŸ¥è¯¢å·²é’ˆå¯¹ PostgreSQL ä¼˜åŒ–

3. **è¿ç§»æ—¶æœº**: ğŸŸ¢ é€‚åˆç«‹å³æ‰§è¡Œ
   - æ•°æ®é‡å°ï¼Œåœæœºæ—¶é—´çŸ­ï¼ˆ< 5 åˆ†é’Ÿï¼‰
   - ç”¨æˆ·å°‘ï¼Œå½±å“èŒƒå›´å°
   - å›¢é˜Ÿå·²æœ‰ PostgreSQL ç»éªŒ

### 11.2 æ¨èä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | ä»»åŠ¡ | æ—¶é—´ | å½±å“ |
|--------|------|------|------|
| **P0ï¼ˆç«‹å³ï¼‰** | 1. é…ç½® PostgreSQL è¿æ¥æ± <br>2. æ·»åŠ  JSONB å­—æ®µä¼˜åŒ–<br>3. é…ç½®è‡ªåŠ¨å¤‡ä»½ | Week 1 | æ€§èƒ½æå‡ 50% |
| **P1ï¼ˆçŸ­æœŸï¼‰** | 1. æ‰§è¡Œ PostgreSQL è¿ç§»<br>2. ä¼˜åŒ–ç´¢å¼•<br>3. é…ç½®ç›‘æ§å‘Šè­¦ | Week 2 | ç¨³å®šæ€§æå‡ |
| **P2ï¼ˆä¸­æœŸï¼‰** | 1. å®æ–½è¯»å†™åˆ†ç¦»<br>2. åˆ›å»ºåˆ†åŒºè¡¨<br>3. é…ç½® Redis ç¼“å­˜ | Month 2-3 | æ”¯æŒ 10 ä¸‡ç”¨æˆ· |
| **P3ï¼ˆé•¿æœŸï¼‰** | 1. å®æ–½æ°´å¹³åˆ†ç‰‡<br>2. ä¼˜åŒ–å½’æ¡£ç­–ç•¥<br>3. å®æ–½ PITR å¤‡ä»½ | Month 6+ | æ”¯æŒ 100 ä¸‡ç”¨æˆ· |

### 11.3 ä¸‹ä¸€æ­¥è¡ŒåŠ¨

#### æœ¬å‘¨è¡ŒåŠ¨ï¼ˆWeek 1ï¼‰
1. **DevOps / SRE**:
   - å®‰è£… PostgreSQL 17ï¼ˆæµ‹è¯•ç¯å¢ƒï¼‰
   - é…ç½®æ€§èƒ½å‚æ•°
   - åˆ›å»ºæ•°æ®åº“å’Œç”¨æˆ·

2. **DB Architectï¼ˆä½ ï¼‰**:
   - ç¼–å†™è¿ç§»è„šæœ¬
   - ä¼˜åŒ–è¡¨ç»“æ„ï¼ˆJSONBã€TIMESTAMPTZï¼‰
   - é…ç½®å¤‡ä»½ç­–ç•¥

3. **Backend Lead**:
   - å®¡æŸ¥åº”ç”¨é…ç½®ï¼ˆdatabase.pyï¼‰
   - éªŒè¯è¿æ¥æ± é…ç½®
   - ç¼–å†™æ•°æ®éªŒè¯è„šæœ¬

#### ä¸‹å‘¨è¡ŒåŠ¨ï¼ˆWeek 2ï¼‰
1. åœ¨æµ‹è¯•ç¯å¢ƒæ‰§è¡Œè¿ç§»
2. è¿è¡Œæ€§èƒ½æµ‹è¯•
3. éªŒè¯æ•°æ®ä¸€è‡´æ€§
4. å‡†å¤‡ç”Ÿäº§ç¯å¢ƒè¿ç§»

---

## é™„å½•

### A. ç›¸å…³æ–‡æ¡£
- [PostgreSQL 17 å®˜æ–¹æ–‡æ¡£](https://www.postgresql.org/docs/17/)
- [SQLAlchemy è¿æ¥æ± é…ç½®](https://docs.sqlalchemy.org/en/20/core/pooling.html)
- [pg_dump å¤‡ä»½æŒ‡å—](https://www.postgresql.org/docs/17/backup-dump.html)

### B. è¿ç§»è„šæœ¬
- `/backend/scripts/migrate_to_postgresql.sh`
- `/backend/scripts/create_tables_optimized.sql`
- `/backend/scripts/create_indexes_optimized.sql`
- `/backend/scripts/backup_postgresql.sh`

### C. è”ç³»æ–¹å¼
- **DB Architect**: database-team@helloagents.com
- **DevOps Lead**: devops@helloagents.com
- **Backend Lead**: backend-team@helloagents.com

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2026-01-10
**ä¸‹æ¬¡å®¡æŸ¥æ—¶é—´**: 2026-02-10ï¼ˆè¿ç§»å 1 ä¸ªæœˆï¼‰
